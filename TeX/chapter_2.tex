\chapter{Математическая модель}
\section{Математическая модель вкраплений на основе схемы независимых испытаний}

\vspace*{1cm}
Контейнер представляет собой последовательность случайных величин распределенных по закону Бернулли с параметром p:
\begin{equation}\label{container:eq}\
\mathcal{L}{x_t} = Bi(1, p), x_i \in V = {0, 1}, i = \overline{1,T};
\end{equation}
Вкрапляемое сообщение имеет вид:
\begin{equation}
\mathcal{L}{m_t} = Bi(1, \theta), m_i \in V = {0, 1}, i = \overline{1,\tau};
\end{equation}
Ключ $\gamma_t$ определяет момент времени вкрапления i-того бита сообщения в исходный контейнер:
\begin{equation}
\mathcal{L}{\gamma_t} = Bi(1, \delta), \gamma_i \in V = {0, 1}, i = \overline{1,T};
\end{equation}
Вкрапление битов ${m_t}$ производится по правилу, заданному следующим функциональным преобразованием:
\begin{equation}\label{input:rule}
y_t=(1-\gamma_t)x_t+\gamma_t m_{\tau_t};
\end{equation}

\begin{lemma}\label{lemma:1}
	Для модели (\ref{container:eq})-(\ref{input:rule})
	\begin{equation}
	P\{y_t=1\}=(1-\delta)p+\delta\theta;
	\end{equation}
	\begin{equation}
	P\{y_t=0\} = (1-\delta)(1-p)+ \delta(1-\theta);
	\end{equation}
\end{lemma}
\begin{proof}
	Воспользуемся формулой полной вероятности:\\
	$P\{y_t=1\}=P\{(1-\gamma_t)x_t + \gamma_t m_{\tau_t} =1\} = \sum_{j \in V} P\{y_t = 1, \gamma_t = j\} =\sum_{j \in V} P\{\gamma_t = j\} P\{y_t = 1|\gamma_t = j\} = (1-\delta)P\{x_=1, \gamma_t=0\} + \delta P\{m_{\tau_t}=1, \gamma_t =1\} = (1-\delta)p + \delta\theta;\\$
	Тогда:\\
	$P\{y_t = 0\} = 1 - P\{y_t = 1\}= (1-\delta)(1-p)+\delta(1-\theta);$
\end{proof} 
\begin{equation}
h = \frac{H(y_1,...,y_t)}{T} = \frac{TH(y_1)}{T}=H(y_1);
\end{equation}
Воспользуемся леммой \ref{lemma:1}:\newline
$
h = -P\{y_t=1\}\log_2 P(y_t = 1)-P\{y_t=0\}\log_2 P(y_t = 0) = - ((1-\delta)p+\delta\theta)\log_2 ((1-\delta)p+\delta\theta) - ((1-\delta)(1-p) + \delta(1-\theta))\log_2((1-\delta)(1-p) + \delta(1-\theta)).
$



\newpage
\section{Математическая модель вкраплений в двоичную стационарную марковскую последовательность 1-го порядка и ее свойства}

Рассмотрим модель (\ref{container:eq})-(\ref{input:rule}).

Пусть контейнер (\ref{container:eq}) пердставляет собой цепь Маркова 1-го порядка с вектором распределения вероятностей $\pi = (\frac{1}{2}, \frac{1}{2})$ и матрицей вероятностей одношаговых переходов
\begin{equation}\label{markov:rule} 
P(\varepsilon)=\frac{1}{2}\bigl( \begin{matrix}
1+\varepsilon,  1-\varepsilon\\
1-\varepsilon,  1+\varepsilon
\end{matrix}\bigl), |\varepsilon|<1, \varepsilon \neq 0.
\end{equation}
\begin{lemma}
	Для модели (\ref{container:eq})-(\ref{input:rule}) с условием (\ref{markov:rule}):
	\begin{equation}\label{1:1}
	P\{y_{t-1}=1, y_t = 1 \}=\frac{1}{4}(1+\varepsilon)(1-\delta)^2+\theta\delta(1-\delta)+\theta^2\delta^2;
	\end{equation}
	\begin{equation}\label{0:1}
	P\{y_{t-1}=1, y_t = 0 \}=\frac{1}{4}(1-\varepsilon)(1-\delta)^2+\frac{1}{2}\delta(1-\delta)+\theta(1-\theta)\delta^2;
	\end{equation}
	\begin{equation}
	P\{y_{t-1}=0, y_t = 1 \}=\frac{1}{4}(1-\varepsilon)(1-\delta)^2+\frac{1}{2}\delta(1-\delta)+\theta(1-\theta)\delta^2;
	\end{equation}
	\begin{equation}\label{0:0}
	P\{y_{t-1}=0, y_t = 0 \}=\frac{1}{4}(1+\varepsilon)(1-\delta)^2+\delta(1-\theta)(1-\delta)+\delta^2(1-\theta)^2.
	\end{equation}
\end{lemma}
\begin{proof}
	Рассмотрим биграмм: $\{y_{t-1}, y_t\}$\\
	$(a_1, a_2) \in \{0,1\}, P\{y_{t-1}=a_1, y_t=a_2\} = \sum_{(b_1, b_2)\in \{0, 1\}^2} P\{y_{t-1} = b_1, y_t = b_2, \gamma_{t-1}=a_1, \gamma_t = a_2\}= \sum_{(b_1, b_2)\in \{0, 1\}^2} P\{y_{t-1} = b_1, y_t = b_2| \gamma_{t-1}=a_1, \gamma_t = a_2\}P\{\gamma_{t-1}=a_1, \gamma_t = a_2\}.$\\
	Для (\ref{1:1}):\\
	$\sum_{(b_1, b_2)\in \{0, 1\}^2} P\{y_{t-1} = b_1, y_t = b_2| \gamma_{t-1}=a_1, \gamma_t = a_2\}P\{\gamma_{t-1}=a_1, \gamma_t = a_2\}=\frac{1}{2}\cdot\frac{1}{2}(1+\varepsilon)(1-\delta)^2+\theta\delta(1-\theta) + \theta^2\delta^2.$\\
	Для случаев (\ref{0:1})-(\ref{0:0}) доказывается аналогично.	
\end{proof}
Для формул (\ref{1:1})-(\ref{0:0}) справедливо условие нормировки:\\
$ \sum_{(a_1, a_2)\in \{0, 1\}^2}P\{y_{t-1}=a_1, y_t=a_2\} =1.$\\
Далее полагаем, что $\theta=\frac{1}{2}$.\\
Тогда: 
\begin{equation}
P\{y_{t-1}=1, y_t = 1 \}=P\{y_{t-1}=0, y_t = 0 \}= \delta^2\frac{\varepsilon}{4}-\delta\frac{\varepsilon}{2}+\frac{1+\varepsilon}{4};
\end{equation}
\begin{equation}
P\{y_{t-1}=1, y_t = 0 \}=P\{y_{t-1}=0, y_t = 1 \}= -\delta^2\frac{\varepsilon}{4}+\delta\frac{\varepsilon}{2}+\frac{1-\varepsilon}{4}.
\end{equation}
\begin{definition}
	\cite{duhin} Дискретный стационарный источник называется марковским источником порядка m, если для любого l(l>m) и любой последовательности $c_l=(a_{i_1}, ..., a_{i_l})$ выполняется: $P\{a_{i_l}|a_{i_{l-1}}, ..., a_{i_1}\} = P\{a_{i_l}|a_{i_{l-1}}, ..., a_{i_{l-m+1}}\}.$
\end{definition}
\begin{definition}
	\cite{duhin} Величина: 
	$H^{(k)} = \sum_{\{c_k\}}P\{a_{i_1}, ..., a_{i_k}\} \log{P\{a_{i_k}|a_{i_{k-1}} ..., a_{i_1}\} }$ называется шаговой энтропией марковского источника порядка k.
\end{definition}

Введем понятие энтропии на знак для l-граммы:\\
\begin{equation}
\label{entropy L}
H_l(\delta) = -\frac{1}{l} \sum_{(a_1, ..., a_l)\in \{0, 1\}}P\{y_{t-l}=a_1,..., y_{t-1}=a_l\}\log{P\{y_{t-l}=a_1, ..., y_{t-1}=a_l\}}.
\end{equation}
При $\delta = 0$ стегоконтейнер $y$ совпадает с контейнером $x$, тогда:\\
\begin{equation}
H_l(0)=-\frac{1}{l}(H\{x_1\}+(l-1)H\{x_2|x_1\});
\end{equation}
\begin{equation}
\lim_{l \to \infty}H_l(0)= \lim_{l \to \infty}-\frac{1}{l}(H\{x_1\}+(l-1)H\{x_2|x_1\}) = H\{x_2|x_1\}.
\end{equation}

Рассмотрим случайную величину $\xi \in B=\{b_1,...,b_m\}$, заданную  на вероятностном пространстве $(\Omega,\mathcal{F},\mathcal{P} )$, $P\{\xi=b_i\}=p_i$.

\begin{definition} \cite{duhin} 
	Величина 
	\begin{equation}\label{own:information}I\{b_i\}=-\log p_i
	\end{equation}
	называется собственной информацией, содержащейся в исходе $b_i \in B$. 
\end{definition}
Велиина $I\{b_i\}$ изменяется от нуля в случае
реализации достоверного исхода до бесконечности, когда $p(b_i)=p_i\rightarrow 0$.
Величину $I\{b_i\}$ можно интерпретировать как априорную неопределённость события $\{\xi=b_i\}$.

Случайная величина $I\{\xi\}$ имеет математическое ожидание
\begin{equation}
EI\{\xi\}=-\sum_{b_i \in B}p_i\log p_i.
\end{equation}

\begin{definition}
	Величина $EI\{\xi\}$ называется средней собственной информацией.
\end{definition}
Cредняя собственная информация равна энтропии: $EI\{\xi\}=H\{\xi\}$.

Для представления функции логарифма воспользуемся формулой Маклорена первого порядка:
\begin{equation}\label{macklaren}
f(\delta)=f(\delta_0)+(\delta-\delta_0)f'(\delta_0)+o((\delta-\delta_0)^2).
\end{equation}

Для краткости, в обозначении функции $\log$ будем использовать основание $b$. 

Согласно (\ref{macklaren}) в точке $\delta=0$ справедливо асимптотическое разложение 1-го порядка:
\begin{gather*}
\log_b(a_0(\varepsilon)+\delta a_1(\varepsilon) + \delta^2 a_2(\varepsilon) + o(\delta^2)) = \\ =
\log a_0(\varepsilon)+\delta \left(\log(a_0(\varepsilon)+\delta a_1(\varepsilon) + \delta^2 a_2(\varepsilon) + o(\delta^2))\right)'|_{\delta=0} + o(\delta)= \\=
\log a_0(\varepsilon) +\delta \frac{1}{\ln b}\frac{a_1(\varepsilon)}{a_0(\varepsilon)}+o(\delta).
\end{gather*}

Учитывая (\ref{macklaren}), найдем асимптотические выражения при $\delta \rightarrow 0$ для собственной информации $I\{y_{t-1}=i_1, y_t = i_2 \}, i_1,i_2 \in \{0,1\}$:
\begin{gather*}
I\{y_{t-1}=0, y_t = 0 \}=I\{y_{t-1}=1, y_t = 1 \}= -\log(\frac{1}{4}(1+\varepsilon)(1-\delta)^2+\frac{1}{2}\delta(1-\delta)+\\\frac{1}{4}\delta^2) =
-\log \frac{1+\varepsilon}{4} + \delta \frac{1}{\ln b}\frac{2\varepsilon}{1+\varepsilon} + o(\delta);
\\
I\{y_{t-1}=0, y_t = 1 \}=I\{y_{t-1}=1, y_t = 0 \}= -\log(\frac{1}{4}(1-\varepsilon)(1-\delta)^2+\\\frac{1}{2}\delta(1-\delta)+\frac{1}{4})\delta^2) = 
-\log \frac{1-\varepsilon}{4} - \delta \frac{1}{\ln b}\frac{2\varepsilon}{1-\varepsilon} + o(\delta).
\end{gather*}

\begin{lemma}
	Если имеет место монобитная модель вкраплений (\ref{container:eq})-(\ref{input:rule}), то для энтропии при $l=2$ справедливо асимптотическое разложение 1-го порядка
	\begin{equation}
	H_2(\delta) = H_2(0) + 2\delta\varepsilon\log\frac{1+\varepsilon}{1-\varepsilon}+O(\delta^2).
	\end{equation}
\end{lemma}
\begin{proof}
	$H_2(\delta)= -( P\{y_{t-1}=0, y_t = 0\}\log(P\{y_{t-1}=0, y_t = 0\}) + P\{y_{t-1}=0, y_t = 1\}\log(P\{y_{t-1}=0, y_t = 1\}) +P\{y_{t-1}=1, y_t = 0\}\log(P\{y_{t-1}=1, y_t = 0\}) +P\{y_{t-1}=1, y_t = 1\}\log(P\{y_{t-1}=1, y_t = 1\})) = -2 (P\{y_{t-1}=0, y_t = 0\}\log(P\{y_{t-1}=0, y_t = 0\}) +P\{y_{t-1}=0, y_t = 0\}\log(P\{y_{t-1}=0, y_t = 1\}) )=-2((\delta^2\frac{\varepsilon}{4}-\delta\frac{\varepsilon}{2}+\frac{1+\varepsilon}{4})(-\log(\frac{1+\varepsilon}{4})+ \delta \frac{1}{\ln b}\cdot \frac{2\varepsilon}{1+\varepsilon}+o(\delta^2)) + (-\delta^2\frac{\varepsilon}{4}+\delta\frac{\varepsilon}{2}+\frac{1-\varepsilon}{4})(-\log(\frac{1-\varepsilon}{4}) - \delta \frac{1}{\ln b}\cdot \frac{2\varepsilon}{1-\varepsilon}+o(\delta^2)))=\frac{1}{2}(-(1+\varepsilon) \log(\frac{1+\varepsilon}{4}) - (1-\varepsilon)\log(\frac{1-\varepsilon}{4}) + 2\delta\varepsilon\log(\frac{1+\varepsilon}{1-\varepsilon})))+O(\delta^2) =H_2(0) + 2\delta\varepsilon\log(\frac{1+\varepsilon}{1-\varepsilon})))+O(\delta^2).$
\end{proof}

\begin{definition}\cite{duhin}
	Величина $\lim_{k\to \infty}H^{(k)}= \lim_{k\to \infty}H_{k}=H_{\infty} \geqslant 0$  называется энтропий марковского источника, где $H_{k}$ - энтропия на знак.
\end{definition}

Рассмотрим  условные вероятности появления трехграммы $(0,0,0)$ при условии всевозможных стегоключей.

$P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0\} =  (1-\delta)^3P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0 |\gamma_{i-1}=0,\gamma_i=0,\gamma_{i+1}=0\} +\newline + 
\delta(1-\delta)^2 (P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0 |\gamma_{i-1}=1,\gamma_i=0,\gamma_{i+1}=0\} + P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0 |\gamma_{i-1}=0,\gamma_i=1,\gamma_{i+1}=0\} + P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0 |\gamma_{i-1}=0,\gamma_i=0,\gamma_{i+1}=1\})+\newline 
+\delta^2(1-\delta) (P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0 |\gamma_{i-1}=1,\gamma_i=1,\gamma_{i+1}=0\}  + P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0 |\gamma_{i-1}=0,\gamma_i=1,\gamma_{i+1}=1\} + P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0 |\gamma_{i-1}=1,\gamma_i=0,\gamma_{i+1}=1\}) + \newline 
+ \delta^3 (P\{y_{i-1} = 0, y_i = 0, y_{i+1} =0 | \gamma_{i-1}=1,\gamma_i=1,\gamma_{i+1}=1\}).$\newline



$P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0|\gamma_1=0,\gamma_2=0,\gamma_3=0\} = P\{x_{i-1} = 0, x_i = 0, x_{i+1} = 0\} = P\{x_{i-1} = 0\}P\{x_i=0, x_{i+1}=0 | x_{i-1} = 0\} = P\{x_{i-1} = 0\}P\{x_i=0\}P\{ x_{i+1}=0 | x_{i} = 0\}  = \frac{1}{2}\cdot\frac{1}{2}(1+\varepsilon)\cdot\frac{1}{2}(1+\varepsilon)=\frac{1}{8}(1+\varepsilon)^2;$\newline

$P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0|\gamma_1=1,\gamma_2=0,\gamma_3=0\} = P\{\xi = 0, x_i = 0, x_{i+1} = 0\} = P\{\xi = 0\}P\{x_i=0, x_{i+1}=0 \} = P\{\xi = 0\}P\{x_i=0\}P\{ x_{i+1}=0 | x_{i} = 0\}  = \frac{1}{2}\cdot\frac{1}{2}(1+\varepsilon)\cdot\frac{1}{2}=\frac{1}{8}(1+\varepsilon);$\newline 

$P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0|\gamma_1=0,\gamma_2=1,\gamma_3=0\} = P\{x_{i-1} = 0, \xi = 0, x_{i+1} = 0\} = P\{\xi = 0\}P\{x_{i-1}=0, x_{i+1}=0 \}  = \frac{1}{2}\cdot\frac{1}{2}\cdot\frac{1}{2}(1+\varepsilon^2)=\frac{1}{8}(1+\varepsilon^2);$\newline


$P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0|\gamma_1=0,\gamma_2=0,\gamma_3=1\} = P\{x_{i-1} = 0, x_i = 0, \xi = 0\} = P\{\xi = 0\}P\{x_{i-1}=0, x_{i}=0 \}  = \frac{1}{2}\cdot\frac{1}{2}\cdot\frac{1}{2}(1+\varepsilon)=\frac{1}{8}(1+\varepsilon);$\newline

$P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0|\gamma_1=0,\gamma_2=1,\gamma_3=1\} = P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0|\gamma_1=1,\gamma_2=0,\gamma_3=1\} =
P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0|\gamma_1=1,\gamma_2=1,\gamma_3=0\} =
P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0|\gamma_1=1,\gamma_2=1,\gamma_3=1\} = P\{\xi = 0, \xi = 0, \xi = 0\} = P\{\xi = 1\}P\{x_{i-1}=1\}P\{x_{i}=0 \}  = \frac{1}{2}\cdot\frac{1}{2}\cdot\frac{1}{2} = \frac{1}{8};$\newline

$P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0\} =  \frac{1}{8}\bigr(\varepsilon(\varepsilon+2)\delta^2 - 2\varepsilon(\varepsilon+1)\delta + (1+\varepsilon)^2 \bigr).$\newline

Найдем вероятности появления всевозможных трехграмм в стегоконтейнере $\{y_t\}$:
\begin{gather*}
P\{y_{i-1} = 1, y_i = 1, y_{i+1} = 1\} = P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0\}=\\=\tfrac{1}{8}\bigr(\varepsilon(\varepsilon+2)\delta^2 - 2\varepsilon(\varepsilon+2)\delta + (1+\varepsilon)^2 \bigr),\\
P\{y_{i-1} = 1, y_i = 1, y_{i+1} = 0\} = \\ = P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 1\}= P\{y_{i-1} = 0, y_i = 1, y_{i+1} = 1\} =\\= P\{y_{i-1} = 1, y_i = 0, y_{i+1} = 0\}   =\tfrac{1}{8}\bigr(-\varepsilon^2\delta^2 + 2\varepsilon^2\delta  -\varepsilon^2 + 1\bigr), \\
P\{y_{i-1} = 1, y_i = 0, y_{i+1} = 1\} = P\{y_{i-1} = 0, y_i = 1, y_{i+1} = 0\}=\\=\tfrac{1}{8}\bigr(\varepsilon(\varepsilon-2)\delta^2 - 2\varepsilon(\varepsilon-2)\delta + (1-\varepsilon)^2 \bigr).
\end{gather*}

\begin{theorem}
	Если имеет место монобитная модель вкраплений (\ref{container:eq})-(\ref{input:rule}) , то для энтропии при $l=3$ справедливо асимптотическое разложение 1-го порядка:
	\begin{equation}
	H_3(\delta)=H_3(0)+2\varepsilon\delta \log\frac{1+\varepsilon}{1-\varepsilon}+ O(\delta^2);
	\end{equation}
	собственная информация имеет вид:
	\begin{gather*}
	I\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0\}=-\log\frac{(1+\varepsilon)^2}{8}+\delta\frac{1}{\ln b}\cdot\frac{2\varepsilon^2+4\varepsilon}{(1+\varepsilon)^2} + O(\delta^2), \\
	I\{y_{i-1} = 1, y_i = 0, y_{i+1} = 0\}= I\{y_{i-1} = 0, y_i = 0, y_{i+1} = 1\}=\\=
	-\log\frac{1-\varepsilon^2}{8}-\delta\frac{1}{\ln b}\cdot\frac{2\varepsilon^2}{1-\varepsilon^2} + O(\delta^2),\\
	I\{y_{i-1} = 0, y_i = 1, y_{i+1} = 0\}= -\log\frac{(1-\varepsilon)^2}{8}+\delta\frac{1}{\ln b}\cdot\frac{2\varepsilon^2-4\varepsilon}{(1-\varepsilon)^2} + O(\delta^2);\\
	I\{y_{i-1} = j_{1}, y_i = j_2, y_{i+1} = j_{3}\}=I\{y_{i-1} = 1-j_{1}, y_i = 1-j_2, y_{i+1} = 1-j_{3}\},\\ ~j_1,j_2,j_3 \in \{0,1\}.
	\end{gather*}		
\end{theorem}
\begin{proof}
	Подставляя в (\ref{macklaren}) найденные выражения для вероятностей трехграмм, получим асимптотические выражения для собственной информации.
	
	Используя выражения для собственной информации, получим:\\
	$H_3(\delta)=-2(\frac{1}{8}\bigr(\varepsilon(\varepsilon+2)\delta^2 - 2\varepsilon(\varepsilon+2)\delta + (1+\varepsilon)^2 \bigr)( \log(\frac{(1+\varepsilon)^2}{8})+\delta\frac{1}{\ln b}\cdot\frac{-2\varepsilon^2-4\varepsilon}{(1+\varepsilon)^2}) + 2\frac{1}{8}\bigr(-\varepsilon^2\delta^2 + 2\varepsilon^2\delta  -\varepsilon^2 + 1\bigr)(\log(\frac{1-\varepsilon^2}{8})+\delta\frac{1}{\ln b}\cdot\frac{2\varepsilon^2}{1-\varepsilon^2}) + \frac{1}{8}\bigr(\varepsilon(\varepsilon-2)\delta^2 - 2\varepsilon(2+\varepsilon)\delta + (1-\varepsilon)^2 \bigr)(\log(\frac{(1-\varepsilon)^2}{8})+\delta\frac{1}{\ln b}\cdot\frac{-2\varepsilon^2+4\varepsilon}{(1-\varepsilon)^2})) + O(\delta^2) =-((1-\varepsilon)\log(1-\varepsilon) + (1+\varepsilon)\log(1+\varepsilon)+\log(\frac{1}{8}) + 2\varepsilon\delta \log\frac{1-\varepsilon}{1+\varepsilon})+ O(\delta^2)=H_3(0)- 2\varepsilon\delta \log\frac{1-\varepsilon}{1+\varepsilon}+ O(\delta^2).$
\end{proof}

Рассмотрим  условные вероятности появления четырехграммы $(0,0,0,0)$ при условии всевозможных стегоключей.

$P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0\} =  (1-\delta)^4P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=0,\gamma_i=0,\gamma_{i+1}=0, \gamma_{i+2} = 0\} + 
\delta(1-\delta)^3\biggr(P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=0,\gamma_i=0,\gamma_{i+1}=0, \gamma_{i+2} = 1\} +
P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=0,\gamma_i=0,\gamma_{i+1}=1, \gamma_{i+2} = 0\} +
P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=0,\gamma_i=1,\gamma_{i+1}=0, \gamma_{i+2} = 0\} +
P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=1,\gamma_i=0,\gamma_{i+1}=0, \gamma_{i+2} = 0\}\biggr) +
\delta^2(1-\delta)^2 \biggr(P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=0,\gamma_i=0,\gamma_{i+1}=1, \gamma_{i+2} = 1\} +
P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=0,\gamma_i=1,\gamma_{i+1}=1, \gamma_{i+2} = 0\} +
P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=1,\gamma_i=1,\gamma_{i+1}=0, \gamma_{i+2} = 0\} +
P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=1,\gamma_i=0,\gamma_{i+1}=1, \gamma_{i+2} = 0\}+
P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=0,\gamma_i=1,\gamma_{i+1}=0, \gamma_{i+2} = 1\}+
P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=1,\gamma_i=0,\gamma_{i+1}=0, \gamma_{i+2} = 1\}\biggr)
+\delta^3(1-\delta) \biggr(P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=0,\gamma_i=1,\gamma_{i+1}=1, \gamma_{i+2} = 1\} +
P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=1,\gamma_i=0,\gamma_{i+1}=1, \gamma_{i+2} = 1\} +
P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=1,\gamma_i=1,\gamma_{i+1}=0, \gamma_{i+2} = 1\} +
P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=1,\gamma_i=1,\gamma_{i+1}=1, \gamma_{i+2} = 0\}\biggr)
+ \delta^4 (	P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=1,\gamma_i=1,\gamma_{i+1}=1, \gamma_{i+2} = 1\}).$\newline

$P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=0,\gamma_i=0,\gamma_{i+1}=0, \gamma_{i+2} = 0\}=\frac{(1+\varepsilon)^3}{16};$\newline

$P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=1,\gamma_i=0,\gamma_{i+1}=0, \gamma_{i+2} = 0\}=P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=0,\gamma_i=0,\gamma_{i+1}=0, \gamma_{i+2} = 1\}=\frac{(1+\varepsilon)^2}{16};$\newline

$P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=0,\gamma_i=1,\gamma_{i+1}=0, \gamma_{i+2} = 0\}=P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=0,\gamma_i=0,\gamma_{i+1}=1, \gamma_{i+2} = 0\} =\frac{(1+\varepsilon)(1+\varepsilon^2)}{16};$\newline

$P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=1,\gamma_i=1,\gamma_{i+1}=0, \gamma_{i+2} = 0\}=P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=0,\gamma_i=0,\gamma_{i+1}=0, \gamma_{i+2} = 1\}=P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=1,\gamma_i=0,\gamma_{i+1}=0, \gamma_{i+2} = 1\}=P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=0,\gamma_i=0,\gamma_{i+1}=1, \gamma_{i+2} = 1\}=\frac{1+\varepsilon}{16};$\newline

$P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=0,\gamma_i=1,\gamma_{i+1}=0, \gamma_{i+2} = 1\}=P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=1,\gamma_i=0,\gamma_{i+1}=1, \gamma_{i+2} = 0\}=\frac{1+\varepsilon^2}{16};$\newline

$P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=1,\gamma_i=1,\gamma_{i+1}=1, \gamma_{i+2} = 0\}=P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=1,\gamma_i=1,\gamma_{i+1}=0, \gamma_{i+2} = 1\}=P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=1,\gamma_i=0,\gamma_{i+1}=1, \gamma_{i+2} = 1\}=P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=0,\gamma_i=1,\gamma_{i+1}=1, \gamma_{i+2} =P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 |\gamma_{i-1}=1,\gamma_i=1,\gamma_{i+1}=1, \gamma_{i+2}= 1\}=\frac{1}{16};$\newline

$P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 \}=P\{y_{i-1} = 1, y_i = 1, y_{i+1} = 1, y_{i+2} = 1 \}= \tfrac{1}{16}\bigr(\delta^4\varepsilon^2+\delta^3(-4\varepsilon^2)+\delta^2(\varepsilon^3+8\varepsilon^2+3\varepsilon)+\delta(-2\varepsilon^3-8\varepsilon^2-6\varepsilon)+\varepsilon^3+3\varepsilon^2+3\varepsilon+1\bigr).$\\
Найдем вероятности появления всевозможных четырехграмм в стегоконтейнере $\{y_t\}$:
\begin{gather*}
P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 1 \}=P\{y_{i-1} = 1, y_i = 1, y_{i+1} = 1, y_{i+2} = 0 \}=\\
P\{y_{i-1} = 1, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 \}=P\{y_{i-1} = 0, y_i = 1, y_{i+1} = 1, y_{i+2} = 1 \}=\\ \tfrac{1}{16}\bigr(\delta^4(-\varepsilon^2)+\delta^3\cdot4\varepsilon^2+\delta^2(-\varepsilon^3-6\varepsilon^2+\varepsilon)+\delta(2\varepsilon^3+4\varepsilon^2-2\varepsilon)-\varepsilon^3-\varepsilon^2+\varepsilon+1\bigr);\\
P\{y_{i-1} = 0, y_i = 1, y_{i+1} = 0, y_{i+2} = 0 \}=P\{y_{i-1} = 1, y_i = 0, y_{i+1} = 1, y_{i+2} = 1 \}=\\
P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 1, y_{i+2} = 0 \}=P\{y_{i-1} = 1, y_i = 1, y_{i+1} = 0, y_{i+2} = 1 \}=\\ \tfrac{1}{16}\bigr(\delta^4(-\varepsilon^2)+\delta^3\cdot4\varepsilon^2+\delta^2(\varepsilon^3-6\varepsilon^2-\varepsilon)+\delta(-2\varepsilon^3+4\varepsilon^2+2\varepsilon)+\varepsilon^3-\varepsilon^2-\varepsilon+1\bigr);\\
P\{y_{i-1} = 0, y_i = 0, y_{i+1} = 1, y_{i+2} = 1 \}=P\{y_{i-1} = 1, y_i = 1, y_{i+1} = 0, y_{i+2}=0 \}=\\ \tfrac{1}{16}\bigr(\delta^4\varepsilon^2+\delta^3(-4\varepsilon^2)+\delta^2(-\varepsilon^3+4\varepsilon^2+\varepsilon)+\delta(2\varepsilon^3-2\varepsilon)-\varepsilon^3-\varepsilon^2+\varepsilon+1\bigr);\\
P\{y_{i-1} = 0, y_i = 1, y_{i+1} = 1, y_{i+2} = 0 \}=P\{y_{i-1} = 1, y_i = 0, y_{i+1} = 0, y_{i+2} = 1 \}=\\ \tfrac{1}{16}\bigr(\delta^4\varepsilon^2+\delta^3(-4\varepsilon^2)+\delta^2(\varepsilon^3+4\varepsilon^2-\varepsilon)+\delta(-2\varepsilon^3+2\varepsilon)+\varepsilon^3-\varepsilon^2-\varepsilon+1\bigr);\\
P\{y_{i-1} = 1, y_i = 0, y_{i+1} = 1, y_{i+2} = 0 \}=P\{y_{i-1} = 0, y_i = 1, y_{i+1} = 0, y_{i+2} = 1 \}=\\= \tfrac{1}{16}\bigr(\delta^4\varepsilon^2+\delta^3(-4\varepsilon^2)+\delta^2(-\varepsilon^3+8\varepsilon^2-3\varepsilon)+\delta(2\varepsilon^3-8\varepsilon^2+6\varepsilon)\\-\varepsilon^3+3\varepsilon^2-3\varepsilon+1\bigr).
\end{gather*}
\begin{theorem}
	Если имеет место монобитная модель вкраплений (\ref{container:eq})-(\ref{input:rule}) , то для энтропии при $l=4$ справедливо асимптотическое разложение 1-го порядка:
	\begin{equation}
	H_4(\delta)=H_4(0)+\frac{24\varepsilon\delta}{16} \log\frac{1+\varepsilon}{1-\varepsilon}+ O(\delta^2);
	\end{equation}
	собственная информация имеет вид:\\
	\begin{gather*}
	I\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 \}=I\{y_{i-1} = 1, y_i = 1, y_{i+1} = 1, y_{i+2} = 1 \}=\\= -\biggr(\log\frac{(1+\varepsilon)^3}{16} + \delta \frac{-2\varepsilon^3-8\varepsilon^2-6\varepsilon}{(1+\varepsilon)^3\ln b}\biggr) +O(\delta^2) ;\\ 
	I\{y_{i-1} = 0, y_i = 0, y_{i+1} = 0, y_{i+2} = 1 \}=I\{y_{i-1} = 1, y_i = 1, y_{i+1} = 1, y_{i+2} = 0 \}=\\
	I\{y_{i-1} = 1, y_i = 0, y_{i+1} = 0, y_{i+2} = 0 \}=I\{y_{i-1} = 0, y_i = 1, y_{i+1} = 1, y_{i+2} = 1 \}=\\= -\biggr(\log\frac{(1-\varepsilon)(1+\varepsilon)^2}{16} + \delta \frac{2\varepsilon^3+4\varepsilon^2-2\varepsilon}{(1-\varepsilon)(1+\varepsilon)^2\ln b}\biggr) +O(\delta^2);\\
	I\{y_{i-1} = 0, y_i = 0, y_{i+1} = 1, y_{i+2} = 0 \}=I\{y_{i-1} = 1, y_i = 1, y_{i+1} = 0, y_{i+2} = 1 \}=\\
	I\{y_{i-1} = 0, y_i = 1, y_{i+1} = 0, y_{i+2} = 0 \}=I\{y_{i-1} = 1, y_i = 0, y_{i+1} = 1, y_{i+2} = 1 \}=\\=
	-\biggr(\log\frac{(1-\varepsilon)^2(1+\varepsilon)}{16} + \delta \frac{-2\varepsilon^3+4\varepsilon^2+2\varepsilon}{(1-\varepsilon)^2(1+\varepsilon)\ln b}\biggr) +O(\delta^2);\\
		\end{gather*}
		\begin{gather*}
	I\{y_{i-1} = 0, y_i = 0, y_{i+1} = 1, y_{i+2} = 1 \}=I\{y_{i-1} = 1, y_i = 1, y_{i+1} = 0, y_{i+2} = 0 \}=\\= -\biggr(\log\frac{(1-\varepsilon)(1+\varepsilon)^2}{16} + \delta \frac{2\varepsilon^3-2\varepsilon}{(1-\varepsilon)(1+\varepsilon)^2\ln b}\biggr) +O(\delta^2);    \\
	I\{y_{i-1} = 0, y_i = 1, y_{i+1} = 1, y_{i+2} = 0 \}=I\{y_{i-1} = 1, y_i = 0, y_{i+1} = 0, y_{i+2} = 1 \}=\\ -\biggr(\log\frac{(1-\varepsilon)^2(1+\varepsilon)}{16} + \delta \frac{-2\varepsilon^3+2\varepsilon}{(1-\varepsilon)^2(1+\varepsilon)\ln b}\biggr) +O(\delta^2);\\  
	I\{y_{i-1} = 1, y_i = 0, y_{i+1} = 1, y_{i+2} = 0 \}=I\{y_{i-1} = 0, y_i = 1, y_{i+1} = 0, y_{i+2} = 1 \}=\\ -\biggr(\log\frac{(1-\varepsilon)^3}{16} + \delta \frac{2\varepsilon^3-8\varepsilon^2+6\varepsilon}{(1-\varepsilon)^3\ln b}\biggr) +O(\delta^2).
	\end{gather*}
\end{theorem}

\begin{proof}
	Подставляя в (\ref{macklaren}) найденные выражения для вероятностей четырехграмм, получим асимптотические выражения для собственной информации. \\
	Используя выражения для собственной информации, получим:
	
	$H_4(\delta)=-2\biggr(
	\tfrac{1}{16}\bigr(\delta^4\varepsilon^2+\delta^3(-4\varepsilon^2)+\delta^2(\varepsilon^3+8\varepsilon^2+3\varepsilon)+\delta(-2\varepsilon^3-8\varepsilon^2-6\varepsilon)+\varepsilon^3+3\varepsilon^2+3\varepsilon+1\bigr)\bigr(\log\frac{(1+\varepsilon)^3}{16} + \delta \frac{-2\varepsilon^3-8\varepsilon^2-6\varepsilon}{(1+\varepsilon)^3\ln b} +O(\delta^2)\bigr) +
	2\tfrac{1}{16}\bigr(\delta^4(-\varepsilon^2)+\delta^3\cdot4\varepsilon^2+\delta^2(-\varepsilon^3-6\varepsilon^2+\varepsilon)+\delta(2\varepsilon^3+4\varepsilon^2-2\varepsilon)-\varepsilon^3-\varepsilon^2+\varepsilon+1\bigr)\bigr(\log\frac{(1-\varepsilon)(1+\varepsilon)^2}{16} + \delta \frac{2\varepsilon^3+4\varepsilon^2-2\varepsilon}{(1-\varepsilon)(1+\varepsilon)^2\ln b} +O(\delta^2)\bigr) +
	2\tfrac{1}{16}\bigr(\delta^4(-\varepsilon^2)+\delta^3\cdot4\varepsilon^2+\delta^2(\varepsilon^3-6\varepsilon^2-\varepsilon)+\delta(-2\varepsilon^3+4\varepsilon^2+2\varepsilon)+\varepsilon^3-\varepsilon^2-\varepsilon+1\bigr)\bigr(\log\frac{(1-\varepsilon)^2(1+\varepsilon)}{16} + \delta \frac{-2\varepsilon^3+4\varepsilon^2+2\varepsilon}{(1-\varepsilon)^2(1+\varepsilon)\ln b}+O(\delta^2)\bigr) +
	\tfrac{1}{16}\bigr(\delta^4\varepsilon^2+\delta^3(-4\varepsilon^2)+\delta^2(-\varepsilon^3+4\varepsilon^2+\varepsilon)+\delta(2\varepsilon^3-2\varepsilon)-\varepsilon^3-\varepsilon^2+\varepsilon+1\bigr)\bigr(log\frac{(1-\varepsilon)(1+\varepsilon)^2}{16} + \delta \frac{2\varepsilon^3-2\varepsilon}{(1-\varepsilon)(1+\varepsilon)^2\ln b} +O(\delta^2)\bigr) +
	\tfrac{1}{16}\bigr(\delta^4\varepsilon^2+\delta^3(-4\varepsilon^2)+\delta^2(\varepsilon^3+4\varepsilon^2-\varepsilon)+\delta(-2\varepsilon^3+2\varepsilon)+\varepsilon^3-\varepsilon^2-\varepsilon+1\bigr)\bigr(\log\frac{(1-\varepsilon)^2(1+\varepsilon)}{16} + \delta \frac{-2\varepsilon^3+2\varepsilon}{(1-\varepsilon)^2(1+\varepsilon)\ln b}+O(\delta^2)\bigr) +
	\tfrac{1}{16}\bigr(\delta^4\varepsilon^2+\delta^3(-4\varepsilon^2)+\delta^2(-\varepsilon^3+8\varepsilon^2-3\varepsilon)+\delta(2\varepsilon^3-8\varepsilon^2+6\varepsilon)-\varepsilon^3+3\varepsilon^2-3\varepsilon+1\bigr)\bigr(\log\frac{(1-\varepsilon)^3}{16} + \delta \frac{2\varepsilon^3-8\varepsilon^2+6\varepsilon}{(1-\varepsilon)^3\ln b} +O(\delta^2)\bigr) 
	\biggr) = 
	\tfrac{(1+\varepsilon)^3}{16}\log\tfrac{(1+\varepsilon)^3}{16} + 3 \tfrac{(1+\varepsilon)^2(1-\varepsilon)}{16}\log\tfrac{(1+\varepsilon)^2(1-\varepsilon)}{16}+3\tfrac{(1+\varepsilon)(1-\varepsilon)^2}{16}\log\tfrac{(1+\varepsilon)(1-\varepsilon)^2}{16}+ \tfrac{(1-\varepsilon)^3}{16}\log\tfrac{(1-\varepsilon)^3}{16}+\frac{24\varepsilon\delta}{16} \log\frac{1+\varepsilon}{1-\varepsilon}+ O(\delta^2) = H_4(0)+\frac{24\varepsilon\delta}{16} \log\frac{1+\varepsilon}{1-\varepsilon}+ O(\delta^2).$
\end{proof}

Оценим остаточный член для асимптотического выражения энтропии биграммы:
\begin{equation}
r_n(\delta)=\frac{f^{(n+1)}(\overline{\delta})}{(n+1)!}(\delta-\delta_0), где \overline{\delta} \in [\delta_0, \delta].
\end{equation}
Для асимптотического разложения 1-го порядка при $\delta_0=0$ остаточный член имеет вид:\\
\begin{equation}
r_n(\delta)=\frac{f''(\overline{\delta})}{2}\delta,\overline{\delta} \in [0, \delta].
\end{equation}
\begin{center}
	$(\log(P_{00}))''=\frac{-2\varepsilon(\delta^2\varepsilon-2\delta\varepsilon+\varepsilon-1)}{\ln b (\delta^2\varepsilon-2\delta\varepsilon+\varepsilon+1)^2};\newline
	(\log(P_{01}))''=\frac{-2\varepsilon(\delta^2\varepsilon-2\delta\varepsilon+\varepsilon+1)}{\ln b (\delta^2\varepsilon-2\delta\varepsilon+\varepsilon-1)^2};\newline$
	
\end{center}
Тогда остаточный член для $\log(P_{00})=\log(P_{11})$ равен:\\
\begin{equation}\label{ost00}
r_{n_{00}}(\delta)=\frac{\delta}{2}\cdot \frac{-2\varepsilon(\delta^2\varepsilon-2\delta\varepsilon+\varepsilon-1)}{\ln b (\delta^2\varepsilon-2\delta\varepsilon+\varepsilon+1)^2}|_{\delta=\overline{\delta}}\newline
\end{equation}
Остаточный член для $\log(P_{10})=\log(P_{01})$ равен:\\
\begin{equation}\label{ost01}
r_{n_{10}}(\delta)=\frac{\delta}{2}\cdot \frac{-2\varepsilon(\delta^2\varepsilon-2\delta\varepsilon+\varepsilon+1)}{\ln b (\delta^2\varepsilon-2\delta\varepsilon+\varepsilon-1)^2}|_{\delta=\overline{\delta}}\newline
\end{equation}
Тогда на основании (\ref{ost00}) и (\ref{ost01}) остаточный член для энтропиии будет равен:\\
\begin{gather*}
R_n(\delta)=-2(P_{00}r_{n_{00}}(\delta)+P_{10}r_{n_{01}}(\delta))=-2\bigr((\delta^2\frac{\varepsilon}{4}-\delta\frac{\varepsilon}{2}+\frac{1+\varepsilon}{4})\\(\frac{\delta}{2}\cdot \frac{-2\varepsilon(\delta^2\varepsilon-2\delta\varepsilon+\varepsilon-1)}{\ln b (\delta^2\varepsilon-2\delta\varepsilon+\varepsilon+1)^2}|_{\delta=\overline{\delta}}) +\\ (-\delta^2\frac{\varepsilon}{4}+\delta\frac{\varepsilon}{2}+\frac{1-\varepsilon}{4})(\frac{\delta}{2}\cdot \frac{-2\varepsilon(\delta^2\varepsilon-2\delta\varepsilon+\varepsilon+1)}{\ln b (\delta^2\varepsilon-2\delta\varepsilon+\varepsilon-1)^2}|_{\delta=\overline{\delta}})\bigr) \newline
\end{gather*}


\clearpage

\newpage



\section{Линейный дискриминантный анализ}
Линейный дискриминантный анализ (ЛДА), а также связанный с ним линейный дискриминант Фишера — методы статистики и машинного обучения, применяемые для нахождения линейных комбинаций признаков, наилучшим образом разделяющих два или более класса объектов или событий. Полученная комбинация может быть использована в качестве линейного классификатора или для сокращения размерности пространства признаков перед последующей классификацией.
ЛДА тесно связан с дисперсионным анализом и регрессионным анализом, также пытающимися выразить какую-либо зависимую переменную через линейную комбинацию других признаков или измерений. В этих двух методах зависимая переменная — численная величина, а в ЛДА она является величиной номинальной (меткой класса). Помимо того, ЛДА имеет схожие черты с методом главных компонент и факторным анализом, которые ищут линейные комбинации величин, наилучшим образом описывающие данные.
Для использования ЛДА признаки должны быть непрерывными величинами, иначе следует использовать анализ соответствий (англ. Discriminant Correspondece Analysis).\cite{lda}
\subsection{Линейный дискриминантный анализ для случая двух классов}
Для каждого образца объекта или события с известным классом $y$ рассматривается набор наблюдений $x$ (называемых ещё признаками, переменными или измерениями). Набор таких образцов называется обучающей выборкой (или набором обучения, обучением). Задачи классификации состоит в том, чтобы построить хороший прогноз класса y для всякого так же распределённого объекта (не обязательно содержащегося в обучающей выборке), имея только наблюдения $x$.\newline
При ЛДА предполагается, что функции совместной плотности распределения вероятностей $p(\vec{x}|y=1)$ и $p(\vec{x}|y=0)$ - нормальны. В этих предположениях оптимальное байесовское решение - относить точки ко второму классу если отношение правдоподобия ниже некоторого порогового значения $T$: $$(\vec{x}-\vec{\mu}_0)^T\Sigma_{y=0}^{-1}(\vec{x}-\vec{\mu}_0)+\ln{|\Sigma _{y=0}|}-(\vec{x}-\vec{\mu}_1)^T\Sigma _{y=1}^{-1}(\vec{x}-\vec{\mu}_1)-\ln{|\Sigma_{y=0}|}<T $$
Если не делается никаких дальнейших предположений, полученную задачу классификации называют квадратичным дискриминантным анализом (англ. quadratic discriminant analysis, QDA). В ЛДА делается дополнительное предположение о гомоскедастичности (т.е. предполагается, что ковариационные матрицы равны, $\Sigma_{y=0}=\Sigma_{y=1}=\Sigma)$ и считается, что ковариационные матрицы имеют полный ранг. При этих предположениях задача упрощается и сводится к сравнению скалярного произведения с пороговым значением 
$$\vec{\omega}\cdot\vec{x}<c $$
для некоторой константы c, где 
$$\vec{\omega}=\Sigma^{-1}(\vec{\mu_1}-\vec{\mu_0}). $$
Это означает, что вероятность принадлежности нового наблюдения $x$ к классу y зависит исключительно от линейной комбинации известных наблюдений.

\subsection{Результаты линейного дискриминантного анализа}
Линейный дискриминантный анализ применен для классификации последовательностей с вкраплениями и без вкраплений при фиксированном параметре $\varepsilon$.\newline
Пусть имеется последовательность $Y=\{y_1,...,y_T\}$, на основании $Y$ вычисляем $(H_3(\delta), H_4(\delta))$ при фиксированном $\varepsilon$, тогда:\newline
$H_0$: последовательность $Y$ имеет вкрапления\newline
$H_1$: последовательность $Y$ не имеет вкраплений\newline
тогда для $n=n_0+n_1$, (где $n_0$ - количество заведомо пустых последовательностей, $n_1$ - количество последовательностей с вкрапениями) последовательностей можно провести дискриминантный анализ и оценить вероятновсть правильной классификации и мощность критерия. \newline
Тогда:
\begin{equation}
\hat{\alpha} = \frac{n_0-\nu_0}{n_0} - \text{оценка вероятности ошибки первого рода};
\end{equation}
\begin{equation}
\hat{\beta} = \frac{n_1-\nu_1}{n_1} - \text{оценка вероятности ошибки второго рода};
\end{equation}
$\nu_0$ - количество верно отпределенных пустых последовательностей,
$\nu_1$ - количество верно отпределенных последоваательностей с вкраплениями.\newline
Мощность критерия:
\begin{equation}
\hat{w} = \frac{\nu_1}{n_1} 
\end{equation}



\begin{table} [h] 	
	\caption{\label{tab:canonsummary}Результаты дискриминантного анализа при $\varepsilon=0.55$.}
	\begin{center}
		\begin{tabular}{|c|c|c|c|}
			\hline
			$\delta$ &  $\hat{\alpha}$ &  $\hat{\beta}$ & $\hat{w}$ \\
			\hline
			0.03 & 0.42 & 0.31 & 0.69\\ 
			\hline
			0.07 & 0.21 & 0.14 & 0.86\\   		
			\hline
			0.08 & 0.1 & 0.13 & 0.87\\  		
			\hline
			0.09 & 0.14 & 0.08 & 0.92\\  		
			\hline
			0.1 & 0.13 & 0.05 & 0.95\\  		
			\hline
			0.3 & 0.05 & 0.02 & 0.98\\  
			\hline			
		\end{tabular}
	\end{center}
	
\end{table} 

\begin{table} [h] 	
	\caption{\label{tab:canonsummary}Результаты дискриминантного анализа при $\varepsilon=0.15$.}
	\begin{center}
		\begin{tabular}{|c|c|c|c|}
			\hline
			$\delta$ &  $\hat{\alpha}$ &  $\hat{\beta}$ & $\hat{w}$ \\
			\hline
			0.01 & 0.47 & 0.4 & 0.6\\ 
			\hline
			0.03 & 0.3 & 0.2 & 0.8\\   		
			\hline
			0.05 & 0.18 & 0.16 & 0.84\\  		
			\hline
			0.07 & 0.1 & 0.08 & 0.92\\  		
			\hline
			0.1 & 0.06 & 0.05 & 0.95\\  		
			\hline
			0.3 & 0.02 & 0.02 & 0.98\\  
			\hline			
		\end{tabular}
	\end{center}
\end{table} 

\begin{figure}[h]
	\center{\includegraphics[width=0.5\linewidth]{lda.png}}
	\caption{График зависимости энтропии $H_4{(\delta)}$ от $H_3{(\delta)}$ при различных долях вкраплений}
	\label{ris:"lda.png"}
\end{figure}
\clearpage
\subsection{Вывод}
Методом линейного дискриминантного анализа на основании энтропийных характеристик $(H_3(\delta), H_4(\delta))$ можно определить наличие вкраплений в последовательность с вероятностью ошибки второго рода 0.05 при доле вкраплений $\delta \geq 0.1$.





\clearpage
\newpage
\section{Исследование реальных данных на основании изображений в формате JPEG}
Формат файла JPEG (Joint Photographic Experts Group - Объединенная экспертная группа по фотографии, произносится "джейпег) был разработан компанией C-Cube Microsystems как эффективный метод хранения изображений с большой глубиной цвета, например, получаемых при сканировании фотографий с многочисленными едва уловимыми (а иногда и неуловимыми) оттенками цвета. Основное отличие формата JPEG от других форматов состоит в том, что в JPEG используется алгоритм сжатия с потерями (а не алгоритм без потерь) информации. Алгоритм сжатия без потерь так сохраняет информацию об изображении, что распакованное изображение в точности соответствует оригиналу. При сжатии с потерями теряется часть информации об изображении, чтобы достичь большего сжатия. Распакованное изображение JPEG редко соответствует оригиналу абсолютно точно, но очень часто эти различия столь незначительны, что их едва можно (если вообще можно) обнаружить.

Ключевым компонентом работы алгоритма является дискретное косинусное
преобразование. Дискретное косинусное преобразование (\ref{dct}) представляет собой
разновидность преобразования Фурье и, так же как и оно, имеет обратное преобразование.
Графическое изображение можно рассматривать как совокупность пространственных
волн, причем оси X и Y совпадают с шириной и высотой картинки, а по оси Z
откладывается значение цвета соответствующего пикселя изображения. Дискретное
косинусное преобразование позволяет переходить от пространственного представления
картинки к ее спектральному представлению и обратно. Воздействуя на спектральное
представление картинки, состоящее из "гармоник"{}, то есть, отбрасывая наименее
значимые из них, можно балансировать между качеством воспроизведения и степенью
сжатия\cite{jpeg}.
\begin{equation}\label{dct}
Y[u,v]=\frac{1}{\sqrt{2N}}\sum\limits_{i=0}^{N-1}\sum\limits_{j=0}^{N-1}C(i,u)C(j,v)y(i,j),
\end{equation}
\begin{equation*}
\text{где } C(i,u)=A(u)cos(\frac{(2i+1)u\pi}{2N}) - \text{гармоника сигнала,}
\end{equation*}
\begin{equation*}
A(u)=\begin{cases}
\frac{1}{\sqrt{2}},\text{u=0;}\\
1, u\neq0;
\end{cases}
\text{- постоянная составляющая.}
\end{equation*}
Обратное дискретное косинусное преобразование:
\begin{equation}\label{idct}
y(i,j)=\frac{1}{\sqrt{2N}}\sum\limits_{u=0}^{N-1}\sum\limits_{v=0}^{N-1}C(i,u)C(j,v)Y[u,v].
\end{equation}

В получившейся матрице коэффициентов низкочастотные компоненты расположены
ближе к левому верхнему углу, а высокочастотные - справа и внизу. Это важно потому,
что большинство графических образов на экране компьютера состоит из низкочастотной
информации. Высокочастотные компоненты не так важны для передачи изображения.
Таким образом, дискретное косинусное преобразование позволяет определить, какую
часть информации можно безболезненно выбросить, не внося серьезных искажений в
картинку.

Далее происходит квантование коэффициентов ДКП. Здесь каждое число из матрицы  делится на элемент из таблицы квантования, а результат округляется до ближайшего целого:
\begin{equation}
Y^q(u,v)=Round\biggl(\frac{Y(u,v)}{q(u,v)}\biggl), 
\end{equation}
\text{где } $q(u,v)$ - \text{элемент таблицы квантования}

Стандарт JPEG даже допускает использование собственных таблиц квантования, которые, однако, необходимо будет передавать декодеру вместе со сжатыми данными, что увеличит общий размер файла. Понятно, что пользователю сложно самостоятельно подобрать 64 коэффициента, поэтому стандарт JPEG использует два подхода для матриц квантования. Первый заключается в том, что в стандарт JPEG включены две рекомендуемые таблицы квантования: одна для яркости, вторая для цветности. Второй подход заключается в синтезе (вычислении на лету) таблицы квантовании, зависящей от одного параметра , который задается пользователем. Сама таблица строится по формуле:
\begin{equation}
Q(i,j)=1+(i+j)R,
\end{equation}
где $R$ - коэффициент сжатия.

Чем больше коэффициент квантования, тем больше данных теряется, поскольку реальное
ДКП-значение представляется все менее и менее точно. Кроме того, для данных яркости и цветности применяются отдельные таблицы квантования, позволяющие квантовать данные цветности с большими коэффициентами, чем данные яркости. Таким образом, JPEG использует различную чувствительность глаза к яркости и цветности изображения.

Далее полученная матрица $8\times8$ переводится в 64-элементный вектор при помощи 
"зигзаг"\ -сканирования (Рис. \ref{ris:"zigzag,jpg"}). Таким образом, в начале вектора мы получаем коэффициенты матрицы, соответствующие низким частотам, а в конце - высоким. 

Заключительна стадия алгоритма сжатия JPEG - кодирование. Полученный вектор обрабатывается с помощью алгоритмов Хаффмана или арифметического кодирования, в зависимости от реализации.

\begin{figure}[h]\label{zigzag}
	\center{\includegraphics[width=0.5\linewidth]{zigzag.jpg}}
	\caption{"Зигзаг"\ -сканирование матрицы.}
	\label{ris:"zigzag,jpg"}
\end{figure}

\begin{figure}[h]
	\center{\includegraphics[width=0.7\linewidth]{hist1.png}}
	\caption{Гистограмма ДКП коэффициентов jpeg изображения.}
	\label{ris:"hist1.png"}
\end{figure}
